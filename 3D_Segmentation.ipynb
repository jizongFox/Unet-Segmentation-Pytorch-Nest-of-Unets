{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOOgN78amx5C",
    "outputId": "e1a06c7c-433e-45ca-af02-94bf0c076232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.6.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.9/dist-packages (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.9/dist-packages (from nibabel) (1.22.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.9/dist-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install nibabel \n",
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Gz8u15rE0IEs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as ff\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IkcyRhigzqUW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\"if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAJ_VNt0z__D",
    "outputId": "234c4f61-5004-4567-cf1e-62c7a33eed95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 28 13:15:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   33C    P0    24W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEoq82mFxCwu",
    "outputId": "a460b5d2-f95e-48a1-ac54-52a209d39e9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "O62gp8CWGed_"
   },
   "outputs": [],
   "source": [
    "def read_files(root):\n",
    "  train_path_list = os.listdir(root)[1:]\n",
    "  train_path_list.sort()\n",
    "\n",
    "  list_img = []\n",
    "  list_target = []\n",
    "  for i in range(len(train_path_list)):\n",
    "    path = os.path.join(root, train_path_list[i])\n",
    "    patient_path_list = os.listdir(path)\n",
    "    patient_path_list.sort()\n",
    "\n",
    "    for i in range(len(patient_path_list)):\n",
    "      if 'frame' in patient_path_list[i] and 'gt' not in patient_path_list[i]:\n",
    "        list_img.append(patient_path_list[i])\n",
    "      elif 'frame' in patient_path_list[i] and 'gt' in patient_path_list[i]:\n",
    "        list_target.append(patient_path_list[i])\n",
    "\n",
    "  new_patient_path = train_path_list * 2\n",
    "  new_patient_path.sort()\n",
    "  return list_img, list_target, new_patient_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oZRlTYnkGfOE"
   },
   "outputs": [],
   "source": [
    "list_img, list_target, new_patient_path = read_files('/content/drive/MyDrive/database/training')\n",
    "\n",
    "with open('/content/drive/MyDrive/database/train_name/image.csv', 'w') as f:\n",
    "  csv_writer = csv.writer(f)\n",
    "  csv_writer.writerow(['path'])\n",
    "  for i in range(len(list_img)):\n",
    "    \n",
    "    path = os.path.join('/content/drive/MyDrive/database/training', new_patient_path[i], list_img[i])\n",
    "    csv_writer.writerow([path])\n",
    "  f.close()\n",
    "\n",
    "with open('/content/drive/MyDrive/database/train_name/target.csv', 'w') as f:\n",
    "  csv_writer = csv.writer(f)\n",
    "  csv_writer.writerow(['path'])\n",
    "  for i in range(len(list_target)):\n",
    "    \n",
    "    path = os.path.join('/content/drive/MyDrive/database/training', new_patient_path[i], list_img[i])\n",
    "    csv_writer.writerow([path])\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3JE4gD7GOzD0"
   },
   "outputs": [],
   "source": [
    "list_img, list_target, new_patient_path = read_files('/content/drive/MyDrive/database/testing')\n",
    "\n",
    "with open('/content/drive/MyDrive/database/test_name/img.csv', 'w') as f:\n",
    "  csv_writer = csv.writer(f)\n",
    "  csv_writer.writerow(['path'])\n",
    "  for i in range(len(list_img)):\n",
    "    \n",
    "    path = os.path.join('/content/drive/MyDrive/database/testing', new_patient_path[i], list_img[i])\n",
    "    csv_writer.writerow([path])\n",
    "  f.close()\n",
    "\n",
    "with open('/content/drive/MyDrive/database/test_name/target.csv', 'w') as f:\n",
    "  csv_writer = csv.writer(f)\n",
    "  csv_writer.writerow(['path'])\n",
    "  for i in range(len(list_target)):\n",
    "    \n",
    "    path = os.path.join('/content/drive/MyDrive/database/testing', new_patient_path[i], list_img[i])\n",
    "    csv_writer.writerow([path])\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VsO4SN9PMRhb"
   },
   "outputs": [],
   "source": [
    "img_path_img_train = pd.read_csv('/content/drive/MyDrive/database/train_name/image.csv')\n",
    "target_path_target_train = pd.read_csv('/content/drive/MyDrive/database/train_name/target.csv')\n",
    "\n",
    "img_path_img_test = pd.read_csv('/content/drive/MyDrive/database/test_name/img.csv')\n",
    "target_path_target_test = pd.read_csv('/content/drive/MyDrive/database/test_name/target.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aJhPRretKuNc"
   },
   "outputs": [],
   "source": [
    "def read_files(img_path, target_path):\n",
    "  img_num = []\n",
    "  img_list = []\n",
    "  target_list = []\n",
    "  for i in range(len(img_path)):\n",
    "    img = sitk.ReadImage(img_path['path'][i])\n",
    "    img = sitk.GetArrayFromImage(img)\n",
    "    img = img[:, None, :, :]\n",
    "    for i in range(img.shape[0]):\n",
    "      img_i = img[i]\n",
    "      img_list.append(img_i)\n",
    "    img_num.append(img.shape[0])\n",
    "    \n",
    "    target = sitk.ReadImage(target_path['path'][i])\n",
    "    target = sitk.GetArrayFromImage(target)\n",
    "    target = target[:, None, :, :]\n",
    "    for i in range(target.shape[0]):\n",
    "      target_i = target[i]\n",
    "      target_list.append(target_i)\n",
    "  \n",
    "  return img_list, target_list, img_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0lsZy8tzQ8ft"
   },
   "outputs": [],
   "source": [
    "img_list_train, target_list_train, img_num_train = read_files(img_path_img_train, target_path_target_train)\n",
    "img_list_test, target_list_test, img_num_test = read_files(img_path_img_train, target_path_target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ng1X_pCZL72c"
   },
   "outputs": [],
   "source": [
    "class ACDC(Dataset):\n",
    "\n",
    "    def __init__(self, img_list, target_list, img_num, crop_size = 256, padding_size = 100):\n",
    "\n",
    "      self.img_list = img_list\n",
    "      self.target_list = target_list\n",
    "      self.img_num = img_num\n",
    "      self.crop_size = crop_size\n",
    "      self.padding_size = padding_size\n",
    "        \n",
    "    def __len__(self):\n",
    "      \n",
    "      return sum(self.img_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "      img = torch.tensor(self.img_list[idx], dtype=torch.float32)\n",
    "      target = torch.tensor(self.target_list[idx], dtype=torch.float32)\n",
    "\n",
    "      img, target = self.center_crop(img, target, self.crop_size, self.padding_size)\n",
    "      \n",
    "      return img, target\n",
    "    \n",
    "    def center_crop(self, img, target, crop_size, padding_size):\n",
    "      img = F.pad(img, pad=(self.padding_size, self.padding_size, self.padding_size, self.padding_size), mode='constant', value=0)\n",
    "      target = F.pad(target, pad=(self.padding_size, self.padding_size, self.padding_size, self.padding_size), mode='constant', value=0)\n",
    "\n",
    "      img = ff.center_crop(img, crop_size)\n",
    "      target = ff.center_crop(target, crop_size)\n",
    "      \n",
    "\n",
    "      return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ei7aqv-OMc5B"
   },
   "outputs": [],
   "source": [
    "ACDC_train = ACDC(img_list_train, target_list_train, img_num_train)\n",
    "ACDC_test = ACDC(img_list_test, target_list_test, img_num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yShdNOwjP-ns"
   },
   "outputs": [],
   "source": [
    "train_data = DataLoader(ACDC_train, batch_size=2, shuffle=True)\n",
    "test_data = DataLoader(ACDC_test, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0r-1lSdtm_6V"
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = x * psi\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self, img_ch=1, output_ch=4):\n",
    "        super(AttU_Net, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(img_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.q = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.k = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "      \n",
    "      q = self.q(g)\n",
    "      k = self.k(x)\n",
    "\n",
    "      B, C, H, W = q.shape\n",
    "      q = rearrange(q, 'b c h w -> b (h w) c')\n",
    "      k = rearrange(k, 'b c h w -> b (h w) c')\n",
    "      x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "      attn = torch.matmul(q, k.transpose(-1, -2)) * ((H * W) ** -0.5)\n",
    "      attn = self.softmax(attn)\n",
    "\n",
    "      out = torch.matmul(attn, x)\n",
    "      out = rearrange(out, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "\n",
    "      return out\n",
    "\n",
    "\n",
    "class ReAttU_Net(nn.Module):\n",
    "    def __init__(self, img_ch=1, output_ch=4):\n",
    "        super(ReAttU_Net, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(img_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Att5 = Attention(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXHAz9DU2DHu",
    "outputId": "a3bcbe93-f10e-4b67-97f6-32ebee1ea7f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model = ReAttU_Net(img_ch=1, output_ch=10)\n",
    "data = torch.rand(size=[1, 1, 224, 224])\n",
    "print(model(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSdRl_sy3J7f",
    "outputId": "64ee9d67-c0ed-4b21-f74d-3fe71f6ca83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model1 = AttU_Net(img_ch=1, output_ch=10)\n",
    "data = torch.rand(size=[1, 1, 224, 224])\n",
    "print(model1(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CwmnrU_5Hfhk"
   },
   "outputs": [],
   "source": [
    "def make_one_hot(input, num_classes):\n",
    "\n",
    "  shape = np.array(input.shape)\n",
    "  shape[1] = num_classes\n",
    "  shape = tuple(shape) \n",
    "  result = torch.zeros(shape)\n",
    "  result = result.scatter_(1, input, 1)\n",
    "\n",
    "  return result\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DiceLoss, self).__init__()\n",
    "\n",
    "\n",
    "  def forward(self, pred, target):\n",
    "    target = make_one_hot(target, 4)\n",
    "    num = target.size(1)\n",
    "    smooth = 1\n",
    "\n",
    "    m1 = pred.view(num, -1)\n",
    "    m2 = target.view(num, -1)\n",
    "\n",
    "    intersection = (m1 * m2)\n",
    "\n",
    "    score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n",
    "    dice_loss = 1 - score.sum() / num\n",
    "\n",
    "    return dice_loss\n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "  target = make_one_hot(target, 4)\n",
    "  smooth = 1.\n",
    "  num = pred.size(1)\n",
    "  m1 = pred.view(num, -1)\n",
    "  m2 = target.view(num, -1)\n",
    "  intersection = (m1 * m2).sum()\n",
    "  return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9ZVya9l2qBTl"
   },
   "outputs": [],
   "source": [
    "model1 = AttU_Net()\n",
    "model1 = model1.to(device)\n",
    "model2 = ReAttU_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PBqwogP_qBaG"
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "  model.train()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "  loss_function = DiceLoss()\n",
    "\n",
    "  train_loss = 0\n",
    "\n",
    "  for epoch in range(100):\n",
    "    for idx, (image, label) in enumerate(train_data):\n",
    "\n",
    "      image = image.to(device)\n",
    "      label = label.to(device)\n",
    "      print(image.is_cuda,label.is_cuda)\n",
    "\n",
    "      pred = model(image)\n",
    "      print(pred.is_cuda)\n",
    "      loss = loss_function(pred, label)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "    \n",
    "    dice_coeff = dice_coeff(pred, label)\n",
    "\n",
    "    print('Epoch %d/%d, Loss=%f, Dice_score=%f'.format(epoch, 100, train_loss / len(train_data), dice_coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "7zEVgJqaqBcN",
    "outputId": "c9c672ec-0496-4fc9-87cf-909ed04689e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n",
      "True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-55e513123dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-8de49323d85f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6bcf06ba7d21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msmooth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6bcf06ba7d21>\u001b[0m in \u001b[0;36mmake_one_hot\u001b[0;34m(input, num_classes)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_scatter__value)"
     ]
    }
   ],
   "source": [
    "train(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akQ4eejgqBeL",
    "outputId": "d42a6927-981f-41bf-818c-62c12485b34e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, (image, label) in enumerate(train_data):\n",
    "  print(image.shape)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJDSNtSKmefc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etv-Y_Fxmehw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mshc5sdymekX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAmvA7WXmemi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZn8Fd6Cmeog"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HG1lZJHZmeqy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmcn7gU9mesz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zi-UEjnPmevD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6kfwfzDmexH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiLXH0BSmezc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zU0bCzaDme13"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_50VZM6me4D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu-V2RC5me7N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFKb5eEQme9C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJOK9pZFme_D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rszwiXcqmfAr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uwehqo5_4dc7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cu7pM_2N4de_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BerZx7uN4dhI"
   },
   "outputs": [],
   "source": [
    "def get_logger(logdir):\n",
    "  if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "  logname = f'run-{time.strftime('%Y-%m-%d-%H-%M')}.log'\n",
    "  log_file = os.path.join(logdir, logname)\n",
    "\n",
    "  logger = logging.getLogger('train')\n",
    "  logger.setLevel(logging.INFO)\n",
    "  formatter = logging.Formatter('%(asctime)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "  stream_handler = logging.StreamHandler(sys.stdout)\n",
    "  stream_handler.setFormatter(formatter)\n",
    "  logger.addHandler(stream_handler)\n",
    "\n",
    "\n",
    "  file_handler = logging.FileHandler(log_file)\n",
    "  file_handler.setFormatter(formatter)\n",
    "  logger.addHandler(file_handler)\n",
    "\n",
    "  return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjsnzsD44djA"
   },
   "outputs": [],
   "source": [
    "def get_argparser():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  # 日志\n",
    "  parser.add_argument('--tensorboard events', type=str, default='/content/drive/MyDrive/events', help='path to tensorboard events')\n",
    "  parser.add_argument('--logs', type=str, default='/content/drive/MyDrive/logs', help='path to train logs')\n",
    "\n",
    "  # 数据\n",
    "  parser.add_argument('--data_root', type=str, default='/content/drive/MyDrive/database/training', help='path to Dataset')\n",
    "  parser.add_argument('--crop_size', type=int, default=154, help='Crop size of dataset')\n",
    "\n",
    "  #模型\n",
    "  parser.add_argument('--model', type=str, default='AttU_Net', choices=['AttU_Net', 'ReAttU_Net'], help='model name')\n",
    "\n",
    "  # 训练参数\n",
    "  parser.add_argument('--batchsize', type=int, default=100, help='batch_size (default: 100)')\n",
    "  parser.add_argument('--total epoch', type=int, default=500, help='epoch number (default: 500)')\n",
    "  parser.add_argument('--optimizer', type=str, default='adam', choice=['sgd', 'adam'], help='choose optimizer')\n",
    "  parser.add_argument('lr', type=float, default=0.01, help='learning rate (default: 0.01)')\n",
    "  parser.add_argument('lr_policy', type=str, default='poly', choices=['poly', 'step', 'multi_step', 'exponential', 'cosine', 'lambda', 'onecycle'], help='learning rate scheduler policy')\n",
    "  parser.add_argument(\"--weight_decay\", type=float, default=1e-4, help='weight decay (default: 1e-4)')\n",
    "  parser.add_argument(\"--random_seed\", type=int, default=1, help='random seed (default: 1)')\n",
    "\n",
    "  #损失函数\n",
    "  parser.add_argument(\"--loss_type\", type=str, default='cross_entropy', help='loss type (default: False)')\n",
    "\n",
    "  # 显卡\n",
    "  parser.add_argument('--gpu_id', type=str, default='0', help='GPU ID')\n",
    "\n",
    "  # 权重\n",
    "  parser.add_argument('--ckpt', default='/content/drive/MyDrive/weights', type=str, help='restore from ckeckpoint')\n",
    "  parser.add_argument('--resume', action='store_true', default=False)\n",
    "\n",
    "  return parser\n",
    "\n",
    "opts = get_argparser().parse_args()\n",
    "\n",
    "# 日志\n",
    "train_logger = get_logger(opts.logs)\n",
    "\n",
    "writer = SummaryWriter(log_dir=opts.tensorboard_events)\n",
    "\n",
    "def _log_stats_train(results, epoch):\n",
    "  tag_value = {'train_loss': results['train_loss'],\n",
    "          'dice_score': results['dice_score']}\n",
    "  for tag, value in tag_value.items():\n",
    "    writer.add_scalar(tag, value, epoch)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu_id\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_logger.info(f'Conf | Device {device}')\n",
    "\n",
    "def get_dataset(opts):\n",
    "  root = os.path.join(opts.data_root)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vneD8jClqBgY"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
